{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foodie Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business case:** There will be a foodie fair in your city next summer, and the organsers have contacted us because they are interested in performing an analysis of the world wide cuisines. They manage to obtain a dataset from different cuisines all over the world, together with the list of the most common ingredients.\n",
    "\n",
    "Some of our strongest geographic and cultural associations are tied to a region's local foods, so they are interested to know more information regarding them in order to organise the different stands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sklearn \n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Other imports\n",
    "from collections import Counter\n",
    "import requests\n",
    "import imageio\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (my_functions_EDA.py, line 493)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"d:\\anaconda\\envs\\dataanalytic\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3417\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-5ec902b2c680>\"\u001b[1;36m, line \u001b[1;32m4\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from my_functions_EDA import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"D:\\Data Course\\Projects\\Food\\Food-Project\\my_functions_EDA.py\"\u001b[1;36m, line \u001b[1;32m493\u001b[0m\n\u001b[1;33m    jupyter nbconvert --to python \"path/to/notebook.ipynb\"\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'D:\\Data Course\\Projects\\Food\\Food-Project')\n",
    "\n",
    "from my_functions_EDA import *\n",
    "from fx_MLRegression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the name of the file\n",
    "data=json.load(open(r'../dataset/Ex 6.1. foodie_dataset.json'))\n",
    "\n",
    "# and load it\n",
    "#file = open(filename, 'rt')\n",
    "#text = file.read()\n",
    "#file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values in training set\n",
    "print(df.isnull().sum())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring cuisine column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30,15),\n",
    "            'axes.labelsize': 15, \n",
    "            'font.size': 15, \n",
    "            'legend.fontsize': 12, \n",
    "            'axes.titlesize':15,\n",
    "            'figure.facecolor': 'white',\n",
    "            'font.family': ['sans-serif'],\n",
    "            'legend.fancybox': True,\n",
    "            'lines.color': 'C0',\n",
    "            'xaxis.labellocation': 'center',\n",
    "            'xtick.alignment': 'center',\n",
    "            'legend.title_fontsize': 15,\n",
    "            'legend.edgecolor': '0.9',\n",
    "            'animation.ffmpeg_path': 'ffmpeg',\n",
    "           })    \n",
    "\n",
    "sns.barplot(df['cuisine'].value_counts().index,\n",
    "            df['cuisine'].value_counts(),palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df.ingredients.apply(', '.join)\n",
    "df['clean'] = df['clean'].str.replace(',',' hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "for recipe in df['clean']:\n",
    "    for word in recipe.split('hola'):\n",
    "        vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features = 1500)\n",
    "train_vectors = count_vectorizer.fit_transform(df['clean'])\n",
    "#pred_vectors = count_vectorizer.transform(pred[\"text2\"])\n",
    "train_vectors\n",
    "#pred_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to remove stopwords here because the TfidfVectorizer we'll do it for us\n",
    "def clean_text(text):\n",
    "    words = [word.lower() for word in text if word.isalpha()]\n",
    "    return (\" \").join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens=re.split('\\W+',text)\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ingredients'] = df['ingredients'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text']=df['clean'].apply(lambda row : tokenize(row.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    clean_text=[word for word in text if word not in stopwords]\n",
    "    return clean_text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w_stop']=df['tokenized_text'].apply(lambda row : remove_stopwords(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer2(tokenized_text):\n",
    "    lematize_text=[lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "    return lematize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['lemmatizer_text']=df['w_stop'].apply(lambda row : lemmatizer2(row))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.lemmatizer_text[35563])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final'] = df.lemmatizer_text.apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.final.str.contains('chicken')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final']= df['final'].str.replace('hola' ,',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(axis = 1, columns=[\"clean\", \"tokenized_text\",\"w_stop\",\"lemmatizer_text\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Final_DATA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the new Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'Final_DATA.csv'\n",
    "df_1 = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape ...\n",
    "print(\"The dataset has {} rows and {} columns.\".format(*df_1.shape))\n",
    "\n",
    "# ... and duplicates\n",
    "print(\"It contains {} duplicates.\".format(df_1.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.drop(axis = 1, columns=['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_1.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.cuisine.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the Lenght of the Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Len'] = df_1['final'].apply(lambda row: len(row.split(', ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.Len.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,7),\n",
    "            'axes.labelsize': 15, \n",
    "            'font.size': 20, \n",
    "            'legend.fontsize': 12, \n",
    "            'axes.titlesize':15,\n",
    "            'figure.facecolor': 'white',\n",
    "            'font.family': ['sans-serif'],\n",
    "            'legend.fancybox': True,\n",
    "            'lines.color': 'C0',\n",
    "            'xaxis.labellocation': 'center',\n",
    "            'xtick.alignment': 'center',\n",
    "            'legend.title_fontsize': 15,\n",
    "            'legend.edgecolor': '0.9',\n",
    "            'animation.ffmpeg_path': 'ffmpeg',\n",
    "           })    \n",
    "\n",
    "sns.barplot(df_1['Len'].value_counts().index,\n",
    "            df_1['Len'].value_counts(),palette='inferno').set(title='Len Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_1.groupby(['cuisine']).agg({'Len': [np.count_nonzero,np.min,np.max,np.mean,]})\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less = df_1[df_1.Len < 16]\n",
    "print(\"We have\", df_less.shape[0] , \"recipes with less than 16 elements, and We had \" , df_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "sns.displot(df_less.Len[df_1.cuisine == \"italian\"], kde=True, bins =15, color='green')\n",
    "plt.legend([ 'Italian Food'])\n",
    "sns.displot(df_less.Len[df_1.cuisine == \"mexican\"], kde=True, bins =15,  color='coral')\n",
    "plt.legend(['Mexican Food'])\n",
    "sns.displot(df_less.Len[df_1.cuisine == \"southern_us\"], kde=True, bins =15, color='blue')\n",
    "plt.legend([ 'Southern US Food'])\n",
    "\n",
    "plt.title('Distribution Plot for Length of Comments\\n')\n",
    "\n",
    "plt.xlabel('\\nRecipe Length')\n",
    "plt.ylabel('Percentage of Comments\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less.groupby(['cuisine']).agg({'Len': [np.min,np.max,np.mean, np.count_nonzero]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less[df_less.Len < 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Meat - NoMeat // Sugar - NotSugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pattern_searcher(search_str:str, search_list:str):\n",
    "\n",
    "    search_obj = re.search(search_list, search_str)\n",
    "    if search_obj :\n",
    "        return_str = search_str[search_obj.start(): search_obj.end()]\n",
    "    else:\n",
    "        return_str = 'NA'\n",
    "    return return_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meat = [' liver ', 'thigh', 'shrimp' , 'steak',  'chorizo', 'sausage', 'pork', 'kosher', 'turkey', 'fillet', \n",
    "        'fryer',  'fish', 'loin',  'serrano', 'crab', 'beef',  'burger', 'blood', 'prawn',  'chop', 'cod',  \n",
    "        'duck',  'lamb', 'oyster', 'tenderloin', 'lard', 'cutlet', 'clam', 'heart', 'rabe', 'shell',  'meat',  \n",
    "        'salmon', 'ear', 'trout',  \"chicken\", \"pepperoni\", \"bacon\", \" ham \" ,\"veal\",  \"salami\" ,\"drumstick\" , \n",
    "        \"breast\" ,\"goose\", \"burger\",  \"lobster\", \"poultry\", \"tripe\",\"mutton\"]\n",
    "\n",
    "\n",
    "pattern = '|'.join(meat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweet = ['sugar', \"chocolate\", 'honey' ]\n",
    "\n",
    "pattern2 = '|'.join(sweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less['meat'] = df_1['final'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [df_less.meat.value_counts()]\n",
    "a = a[1:11]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,10),\n",
    "            'axes.labelsize': 15, \n",
    "            'font.size': 20, \n",
    "            'legend.fontsize': 12, \n",
    "            'axes.titlesize':15,\n",
    "            'figure.facecolor': 'white',\n",
    "            'font.family': ['sans-serif'],\n",
    "            'legend.fancybox': True,\n",
    "            'lines.color': 'C0',\n",
    "            'xaxis.labellocation': 'center',\n",
    "            'xtick.alignment': 'center',\n",
    "            'legend.title_fontsize': 15,\n",
    "            'legend.edgecolor': '0.9',\n",
    "            'animation.ffmpeg_path': 'ffmpeg',\n",
    "           })    \n",
    "\n",
    "sns.barplot(df_less['meat'].value_counts().index[1:11],\n",
    "            df_less['meat'].value_counts()[1:11],palette='viridis').set(title='Ten Meets most frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less['sugar'] = df_1['final'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomeat = df_less[df_less['meat'] == \"NA\"]\n",
    "nomeat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30,15),\n",
    "            'axes.labelsize': 15, \n",
    "            'font.size': 15, \n",
    "            'legend.fontsize': 12, \n",
    "            'axes.titlesize':15,\n",
    "            'figure.facecolor': 'white',\n",
    "            'font.family': ['sans-serif'],\n",
    "            'legend.fancybox': True,\n",
    "            'lines.color': 'C0',\n",
    "            'xaxis.labellocation': 'center',\n",
    "            'xtick.alignment': 'center',\n",
    "            'legend.title_fontsize': 15,\n",
    "            'legend.edgecolor': '0.9',\n",
    "            'animation.ffmpeg_path': 'ffmpeg',\n",
    "           })    \n",
    "\n",
    "sns.barplot(nomeat['cuisine'].value_counts().index,\n",
    "            nomeat['cuisine'].value_counts(),palette='viridis', ).set(title='Distribution of the NoMeet recipes by cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meat = df_less[df_less['meat'] != \"NA\"]\n",
    "meat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30,15),\n",
    "            'axes.labelsize': 15, \n",
    "            'font.size': 15, \n",
    "            'legend.fontsize': 12, \n",
    "            'axes.titlesize':15,\n",
    "            'figure.facecolor': 'white',\n",
    "            'font.family': ['sans-serif'],\n",
    "            'legend.fancybox': True,\n",
    "            'lines.color': 'C0',\n",
    "            'xaxis.labellocation': 'center',\n",
    "            'xtick.alignment': 'center',\n",
    "            'legend.title_fontsize': 15,\n",
    "            'legend.edgecolor': '0.9',\n",
    "            'animation.ffmpeg_path': 'ffmpeg',\n",
    "           })    \n",
    "\n",
    "sns.barplot(meat['cuisine'].value_counts().index,\n",
    "            meat['cuisine'].value_counts(),palette='viridis').set(title='Distribution of the Meet recipes by cuisine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging the ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a new column with the preprocess\n",
    "df_less[\"prepros\"] = df_less[\"final\"].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jj = []\n",
    "for a in df_less[\"prepros\"]:\n",
    "    for row in a:\n",
    "        if 'JJ' in row:\n",
    "            jj.append(row[0])\n",
    "jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = []\n",
    "for a in df_less[\"prepros\"]:\n",
    "    for row in a:\n",
    "        if 'NN' in row:\n",
    "            nn.append(row[0])\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabjj = Counter(jj)\n",
    "vocabnn = Counter(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabjj.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabnn.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for Marcks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "## Spanish corpus (https://spacy.io/models/es#es_core_news_md)\n",
    "# python -m spacy download es\n",
    "import es_core_news_sm\n",
    "nlp_sp = es_core_news_sm.load()\n",
    "nlp_sp\n",
    "\n",
    "## English corpus\n",
    "#python -m spacy download en\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less[\"prepros2\"] = df_less[\"final\"].apply(lambda x: nlp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for row in df_less[\"ingredients\"]:\n",
    "    doc = nlp(row)\n",
    "    a += [(X.text, X.label_) for X in doc.ents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(X) for X in a if 'ORG' in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering without a Cuisine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(df_less['final'])\n",
    "\n",
    "train_vectors\n",
    "\n",
    "a = train_vectors.todense()\n",
    "\n",
    "unidades_datos = np.array(a)\n",
    "unidades_datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I transform the array into a DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(unidades_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "inertia =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###Static code to get max no of clusters\n",
    "\n",
    "for i in range(1,10):\n",
    "    kmeans = KMeans(n_clusters= i,random_state = 43)\n",
    "    kmeans.fit(new_df)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "kl = KneeLocator(range(1, 10), inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "print('The elbow point of your model is:', kl.elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmodel = KMeans(n_clusters= 4, random_state=0)\n",
    "y_kmeans= kmeansmodel.fit_predict(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less['cluster_id'] = y_kmeans\n",
    "df_less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less['cluster_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2= df_less[df_less['cluster_id'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2['cuisine'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2.groupby(['cuisine'])[\"Len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0= df_less[df_less['cluster_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0.groupby(['cuisine'])[\"Len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0['cuisine'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1= df_less[df_less['cluster_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.groupby(['cuisine'])[\"Len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1= df_less[df_less['cluster_id'] == 1]\n",
    "c2= df_less[df_less['cluster_id'] == 2]\n",
    "c3= df_less[df_less['cluster_id'] == 3]\n",
    "c0= df_less[df_less['cluster_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=200, max_words=20, background_color=\"white\",\n",
    "                      \n",
    "                      width= 300, height = 200,\n",
    "                      stopwords = stopwords.words('english')).generate(str(c0.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'gray' , random_state=17), '\\nWords')\n",
    "wordcloud = WordCloud(max_font_size=200, max_words=20, background_color=\"white\",\n",
    "                      \n",
    "                      width= 300, height = 200,\n",
    "                      stopwords = stopwords.words('english')).generate(str(c1.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'hot' , random_state=17), '\\nWords')\n",
    "wordcloud = WordCloud(max_font_size=200, max_words=20, background_color=\"white\",\n",
    "                      \n",
    "                      width= 300, height = 200,\n",
    "                      stopwords = stopwords.words('english')).generate(str(c2.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'winter' , random_state=17), '\\nWords')\n",
    "wordcloud = WordCloud(max_font_size=200, max_words=20, background_color=\"white\",\n",
    "                      \n",
    "                      width= 300, height = 200,\n",
    "                      stopwords = stopwords.words('english')).generate(str(c3.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'Greens' , random_state=17), '\\nWords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Step 1. Instantiate the model (label encoding)\n",
    "lb_make = LabelEncoder() \n",
    "\n",
    "# Step 2. Fit the variable to the instatiated model\n",
    "new_df['Country'] = lb_make.fit_transform(df_less['cuisine'])\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.Country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "X = new_df\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "#pd.read_csv()\n",
    "scaler\n",
    "\n",
    "df_sc = scaler.fit_transform(X)\n",
    "\n",
    "df_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "inertia =[]\n",
    "\n",
    "###Static code to get max no of clusters\n",
    "\n",
    "for i in range(1,10):\n",
    "    kmeans = KMeans(n_clusters= i,random_state = 43)\n",
    "    kmeans.fit(new_df)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc = pd.DataFrame(df_sc, columns = X.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "kl = KneeLocator(range(1, 10), inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "print('The elbow point of your model is:', kl.elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmodel = KMeans(n_clusters= 6, random_state=0)\n",
    "y_kmeans= kmeansmodel.fit_predict(df_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_less['cluster_id'] = y_kmeans\n",
    "df_less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2.cuisine.value_counst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [i.split(' ') for i in df_less.final if i != ',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [i for i in text_list if i != ',' or ', ']\n",
    "#[variable_name for variable_name in original_list if condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Build our own dictionary, and save it for future use\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, \n",
    "# where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(text_list)#\n",
    "dictionary.save('dictionary.dict')\n",
    "print (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Vectorize the characters\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2. Create and save the gensim Corpus from the processed dictionary\n",
    "corpora.MmCorpus.serialize('corpus_s.mm', doc_term_matrix)\n",
    "\n",
    "print (len(doc_term_matrix))\n",
    "print (doc_term_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3. Perform the LDA model\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, \n",
    "               passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.print_topics(num_topics= 10, num_words = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's save the model for the future \n",
    "ldamodel.save('topic.model')\n",
    "## load saved model\n",
    "from gensim.models import LdaModel\n",
    "model_loaded = LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d = gensim.corpora.Dictionary.load('dictionary.dict')\n",
    "c = gensim.corpora.MmCorpus('corpus_s.mm')\n",
    "lda = gensim.models.LdaModel.load('topic.model')\n",
    "\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(lda, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian = df_less[df_less.cuisine == \"italian\"]\n",
    "mexican = df_less[df_less.cuisine == \"mexican\"]\n",
    "southern_us = df_less[df_less.cuisine == \"southern_us\"]\n",
    "indian = df_less[df_less.cuisine == \"indian\"]\n",
    "chinese = df_less[df_less.cuisine == \"chinese\"]\n",
    "french = df_less[df_less.cuisine == \"french\"]\n",
    "cajun_creole = df_less[df_less.cuisine == \"cajun_creole\"]\n",
    "thai  = df_less[df_less.cuisine == \"thai\"]\n",
    "japanese= df_less[df_less.cuisine == \"japanese\"]\n",
    "greek = df_less[df_less.cuisine == \"greek\"]\n",
    "spanish = df_less[df_less.cuisine == \"spanish\"]\n",
    "korean = df_less[df_less.cuisine == \"korean\"]\n",
    "vietnamese = df_less[df_less.cuisine == \"vietnamese\"]\n",
    "moroccan = df_less[df_less.cuisine == \"moroccan\"]\n",
    "british = df_less[df_less.cuisine == \"british\"]\n",
    "filipino = df_less[df_less.cuisine == \"filipino\"]\n",
    "irish  = df_less[df_less.cuisine == \"irish\"]\n",
    "jamaican = df_less[df_less.cuisine == \"jamaican\"]\n",
    "russian = df_less[df_less.cuisine == \"russian\"]\n",
    "brazilian = df_less[df_less.cuisine == \"brazilian\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita = np.array(Image.open(requests.get('https://img2.freepng.es/20180404/azq/kisspng-italy-vector-map-the-seven-wonders-5ac55fcd89ce32.5470960615228845575645.jpg', stream=True).raw))\n",
    "mex = np.array(Image.open(requests.get('https://cdn1.vectorstock.com/i/1000x1000/55/10/mexico-solid-black-silhouette-map-of-country-vector-21655510.jpg', stream=True).raw))\n",
    "sou = np.array(Image.open(requests.get('https://st2.depositphotos.com/2567911/8292/v/950/depositphotos_82920302-stock-illustration-black-silhouette-map-of-united.jpg', stream=True).raw))\n",
    "\n",
    "\n",
    "ind = np.array(Image.open(requests.get('https://w7.pngwing.com/pngs/693/612/png-transparent-india-blank-map-line-art-monochrome-india-world.png', stream=True).raw))\n",
    "chi =  np.array(Image.open(requests.get('https://cdn2.vectorstock.com/i/1000x1000/63/06/map-of-china-vector-20546306.jpg', stream=True).raw))\n",
    "#french = df_1[df_1.cuisine == \"french\"]\n",
    "#cajun_creole = df_1[df_1.cuisine == \"cajun_creole\"]\n",
    "#thai  = df_1[df_1.cuisine == \"thai\"]\n",
    "#japanese= df_1[df_1.cuisine == \"japanese\"]\n",
    "#greek = df_1[df_1.cuisine == \"greek\"]\n",
    "#spanish = df_1[df_1.cuisine == \"spanish\"]\n",
    "#korean = df_1[df_1.cuisine == \"korean\"]\n",
    "#vietnamese = df_1[df_1.cuisine == \"vietnamese\"]\n",
    "#moroccan = df_1[df_1.cuisine == \"moroccan\"]\n",
    "#british = df_1[df_1.cuisine == \"british\"]\n",
    "#filipino = df_1[df_1.cuisine == \"filipino\"]\n",
    "#irish  = df_1[df_1.cuisine == \"irish\"]\n",
    "#jamaican = df_1[df_1.cuisine == \"jamaican\"]\n",
    "#russian = df_1[df_1.cuisine == \"russian\"]\n",
    "#brazilian = = np.array(Image.open(requests.get('https://i.pinimg.com/originals/f2/94/4f/f2944f132ac16b8dedc74d3f77249420.jpg', stream=True).raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(wordcloud, language):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(language + ' Comments\\n', fontsize=18, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=200, max_words=50, background_color=\"white\",\n",
    "                      mask=ita ,width= 3000, height = 2000,\n",
    "                      stopwords = stopwords.words('english')).generate(str(italian.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'RdYlGn' , random_state=17), '\\nWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=200, max_words=50, background_color=\"white\",\n",
    "                      mask=mex ,width= 3000, height = 2000,\n",
    "                      stopwords = stopwords.words('english')).generate(str(mexican.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'YlGn' , random_state=17), '\\nWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=200, max_words=50, background_color=\"white\",\n",
    "                      mask=sou ,width= 3000, height = 2000,\n",
    "                      stopwords = stopwords.words('english')).generate(str(southern_us.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'RdBu' , random_state=17), '\\nWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=200, max_words=50, background_color=\"white\",\n",
    "                      mask=ind ,width= 3000, height = 2000,\n",
    "                      stopwords = stopwords.words('english')).generate(str(indian.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'RdYlGn' , random_state=17), '\\nWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=200, max_words=50, background_color=\"white\",\n",
    "                      mask=chi ,width= 3000, height = 2000,\n",
    "                      stopwords = stopwords.words('english')).generate(str(chinese.final.values))\n",
    "\n",
    "plot_wordcloud(wordcloud.recolor( colormap= 'hot' , random_state=17), '\\nWords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
